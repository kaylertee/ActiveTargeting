{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1242642",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- Use multimeter so arbitrarily long binders can be generated instead of forcing the AF2 default\n",
    "- Reinforcement Learning \n",
    "- Genetic Algorithm/Evolution so that useful initial changes can be refined instead of randomly restarting a new trajectory\n",
    "- See if hybrid mutation approach is faster/req less compute than BindCraft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9858813",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Currently using Single-Segment so binder lengths are fixed (300)\n",
    "- Makes use of AlphaFold weights, doesn't use BindCraft code/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea00cb",
   "metadata": {},
   "source": [
    "# MutaCraft: Hybrid Seed + De Novo Binder Generation\n",
    "\n",
    "Run this notebook inside your local WSL installation. It swaps the standard binder initialization for a hybrid strategy that mixes a user-provided seed with random exploration, then steps through a four-stage AlphaFold2 optimization (continuous \u2192 softmax \u2192 STE \u2192 semi-greedy).\n",
    "\n",
    "You can point the notebook at any target structure under `InputTargets`, provide a template binder (optional), and supply a seed FASTA string or file. Outputs are written to the `Results` directory so they stay consistent with the rest of the tooling.\n",
    "\n",
    "### Stage Flow\n",
    "- Stage 1 \u2013 Continuous logits with a guided seed ratio anneal plus optional guided KL prior.\n",
    "- Stage 2 \u2013 Temperature annealing down to a hard distribution while the KL weight tapers out.\n",
    "- Stage 3 \u2013 Straight-through (hard) updates for sharper convergence.\n",
    "- Stage 4 \u2013 Semi-greedy mutation search with optional bias toward the seed amino acids at guided sites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d733fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35199/4014229003.py:20: DeprecationWarning: wraps was deprecated in JAX v0.6.0 and will be removed in JAX v0.7.0.\n",
      "  if jax_util is not None and not hasattr(jax_util, \"wraps\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import functools\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import tree_util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure ColabDesign remains compatible with modern JAX releases.\n",
    "if not hasattr(jax, \"tree_map\"):\n",
    "    jax.tree_map = tree_util.tree_map  # type: ignore[attr-defined]\n",
    "try:\n",
    "    import jax.util as jax_util  # type: ignore\n",
    "except ImportError:  # pragma: no cover - optional shim for newer JAX.\n",
    "    jax_util = None\n",
    "if jax_util is not None and not hasattr(jax_util, \"wraps\"):\n",
    "    def _jax_util_wraps(wrapped, *, docstr=None, assigned=None, updated=None):\n",
    "        assigned = (\"__module__\", \"__name__\", \"__qualname__\", \"__doc__\", \"__annotations__\")\n",
    "        updated = (\"__dict__\",)\n",
    "        def decorator(func):\n",
    "            result = functools.wraps(wrapped, assigned=assigned, updated=updated)(func)\n",
    "            if docstr is not None:\n",
    "                result.__doc__ = docstr\n",
    "            return result\n",
    "        return decorator\n",
    "    jax_util.wraps = _jax_util_wraps  # type: ignore[attr-defined]\n",
    "\n",
    "from colabdesign import mk_afdesign_model, clear_mem\n",
    "\n",
    "AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_idx = {a: i for i, a in enumerate(AA)}\n",
    "idx_to_aa = {i: a for a, i in aa_to_idx.items()}\n",
    "\n",
    "ROOT_DIR = Path(r\"/mnt/e/Code/BindCraft\").resolve()\n",
    "if not ROOT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"root dir not found: {ROOT_DIR}\")\n",
    "\n",
    "INPUT_DIR_CANDIDATES = [ROOT_DIR / \"InputTargets\", ROOT_DIR / \"inputtargets\"]\n",
    "for cand in INPUT_DIR_CANDIDATES:\n",
    "    if cand.exists():\n",
    "        INPUT_DIR = cand.resolve()\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"No InputTargets directory found. Expected one of: \" + \", \".join(str(p) for p in INPUT_DIR_CANDIDATES))\n",
    "\n",
    "RESULTS_DIR_CANDIDATES = [ROOT_DIR / \"Results\", ROOT_DIR / \"results\"]\n",
    "for cand in RESULTS_DIR_CANDIDATES:\n",
    "    if cand.exists():\n",
    "        RESULTS_DIR = cand.resolve()\n",
    "        break\n",
    "else:\n",
    "    RESULTS_DIR = (ROOT_DIR / \"Results\").resolve()\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "AF_PARAMS_DIR = (ROOT_DIR / \"bindcraft\" / \"params\").resolve()\n",
    "if not AF_PARAMS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"AlphaFold parameter directory not found: {AF_PARAMS_DIR}\")\n",
    "os.environ.setdefault(\"AF_PARAMS_DIR\", str(AF_PARAMS_DIR))\n",
    "OUTPUT_BASE = (RESULTS_DIR / \"MutaCraft\").resolve()\n",
    "OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GLOBAL_RNG_SEED = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User inputs ---\n",
    "SEED_FASTA = \"\"  # paste binder FASTA directly here (optional if using SEED_FASTA_PATH).\n",
    "SEED_FASTA_PATH = INPUT_DIR / \"seed.fasta\"  # optional FASTA file on disk.\n",
    "TARGET_PDB_PATH = INPUT_DIR / \"HumanLysozyme.pdb\"    # Target Protein to design binders for\n",
    "TEMPLATE_BINDER_PDB_PATH = INPUT_DIR / \"HL6_camel_VHH_fragment.pdb\"  # optional template binder PDB.\n",
    "TEMPLATE_BINDER_CHAINS = [\"A\"]  # chains to pull sequence from when using template binder\n",
    "TARGET_CHAIN = \"B\"  # chain ID in TARGET_PDB_PATH used for receptor\n",
    "\n",
    "L = None  # binder length (None auto-detects from seed/template).\n",
    "GUIDED_FRACTION = 0.5\n",
    "GUIDED_SEED_RATIO_START = 0.9\n",
    "GUIDED_SEED_RATIO_END = 0.2\n",
    "\n",
    "STAGE1_ITERS = 50\n",
    "STAGE1_EXTRA = 25\n",
    "STAGE2_ITERS = 45\n",
    "STAGE3_ITERS = 5\n",
    "STAGE4_ITERS = 15\n",
    "MUT_RATE = 0.05\n",
    "\n",
    "KL_W_START = 0.1\n",
    "KL_W_END = 0.0\n",
    "USE_KL_PRIOR = True\n",
    "BIAS_STAGE4_PROPOSALS = True\n",
    "STAGE4_SEED_BIAS = 0.15  # probability mass reassigned to seed AA when biasing proposals.\n",
    "\n",
    "RUN_NAME = \"HL6_VHH_run\"  # subdirectory within Results/MutaCraft/\n",
    "RUN_BASELINES = False\n",
    "RUN_RANDOM_BASELINE = True\n",
    "RUN_SEED_BASELINE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5790565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta_str(fasta_str: str) -> str:\n",
    "    \"\"\"Parse a FASTA string and return the concatenated sequence.\"\"\"\n",
    "    lines = [line.strip() for line in fasta_str.splitlines() if line.strip()]\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "    if lines[0].startswith(\">\"):\n",
    "        lines = lines[1:]\n",
    "    seq = \"\".join(lines).replace(\" \", \"\").replace(\"\t\", \"\")\n",
    "    return seq.upper()\n",
    "\n",
    "\n",
    "def seq_to_onehot(seq: str) -> jnp.ndarray:\n",
    "    \"\"\"Convert a sequence string to a one-hot (L, 20) array.\"\"\"\n",
    "    L_local = len(seq)\n",
    "    oh = jnp.zeros((L_local, 20))\n",
    "    idxs = jnp.array([aa_to_idx.get(a, -1) for a in seq])\n",
    "    assert jnp.all(idxs >= 0), \"Sequence contains non-standard amino acids.\"\n",
    "    oh = oh.at[jnp.arange(L_local), idxs].set(1.0)\n",
    "    return oh\n",
    "\n",
    "\n",
    "def onehot_to_logits(oh: jnp.ndarray, sharp: float = 6.0) -> jnp.ndarray:\n",
    "    \"\"\"Map one-hot encodings to sharp logits so softmax(logits) is close to one-hot.\"\"\"\n",
    "    return sharp * oh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a378014",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREE_TO_ONE = {\n",
    "    'ALA': 'A',\n",
    "    'ARG': 'R',\n",
    "    'ASN': 'N',\n",
    "    'ASP': 'D',\n",
    "    'CYS': 'C',\n",
    "    'GLN': 'Q',\n",
    "    'GLU': 'E',\n",
    "    'GLY': 'G',\n",
    "    'HIS': 'H',\n",
    "    'ILE': 'I',\n",
    "    'LEU': 'L',\n",
    "    'LYS': 'K',\n",
    "    'MET': 'M',\n",
    "    'PHE': 'F',\n",
    "    'PRO': 'P',\n",
    "    'SER': 'S',\n",
    "    'THR': 'T',\n",
    "    'TRP': 'W',\n",
    "    'TYR': 'Y',\n",
    "    'VAL': 'V',\n",
    "}\n",
    "\n",
    "def extract_pdb_sequence(pdb_path: Path, chains: Optional[Iterable[str]] = None) -> str:\n",
    "    \"\"\"Extract a concatenated sequence from the specified chains in a PDB file.\"\"\"\n",
    "    def _parse_resseq(val: str) -> tuple[int, str]:\n",
    "        val = val.strip()\n",
    "        number = []\n",
    "        suffix = []\n",
    "        for ch in val:\n",
    "            if ch.isdigit() or (ch == '-' and not number):\n",
    "                number.append(ch)\n",
    "            else:\n",
    "                suffix.append(ch)\n",
    "        num_val = int(''.join(number)) if number else 0\n",
    "        return num_val, ''.join(suffix)\n",
    "\n",
    "    if chains is not None:\n",
    "        chain_order = list(chains)\n",
    "    else:\n",
    "        chain_order = []\n",
    "    residues_by_chain: Dict[str, Dict[tuple, str]] = {}\n",
    "    with pdb_path.open() as handle:\n",
    "        for line in handle:\n",
    "            if not line.startswith(('ATOM', 'HETATM')):\n",
    "                continue\n",
    "            chain_id = line[21] if len(line) > 21 else ' ' \n",
    "            chain_id = chain_id if chain_id.strip() else ' ' \n",
    "            if chains is not None and chain_id not in chain_order:\n",
    "                continue\n",
    "            resseq = line[22:26]\n",
    "            icode = line[26]\n",
    "            key = (chain_id, resseq, icode)\n",
    "            chain_dict = residues_by_chain.setdefault(chain_id, {})\n",
    "            if key in chain_dict:\n",
    "                continue\n",
    "            resname = line[17:20].strip().upper()\n",
    "            aa = THREE_TO_ONE.get(resname, 'X')\n",
    "            chain_dict[key] = aa\n",
    "    if chains is None:\n",
    "        chain_order = sorted(residues_by_chain.keys())\n",
    "    sequence_parts = []\n",
    "    for chain_id in chain_order:\n",
    "        residues = residues_by_chain.get(chain_id, {})\n",
    "        sorted_keys = sorted(residues.keys(), key=lambda x: (*_parse_resseq(x[1]), x[2]))\n",
    "        for key in sorted_keys:\n",
    "            sequence_parts.append(residues[key])\n",
    "    return ''.join(sequence_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2294997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_guided_mask(L_local: int, fraction: float = 0.5, rng: Optional[np.random.Generator] = None) -> np.ndarray:\n",
    "    \"\"\"Create a boolean mask marking guided positions.\"\"\"\n",
    "    rng = np.random.default_rng(None if rng is None else rng.integers(1 << 32))\n",
    "    idx = np.arange(L_local)\n",
    "    rng.shuffle(idx)\n",
    "    k = int(round(fraction * L_local))\n",
    "    guided = np.zeros(L_local, dtype=bool)\n",
    "    guided[idx[:k]] = True\n",
    "    return guided\n",
    "\n",
    "\n",
    "def guided_seed_ratio_schedule(t: int, t_max: int, a0: float = 0.9, a1: float = 0.2) -> float:\n",
    "    \"\"\"Linear decay from a0 to a1 over t_max steps.\"\"\"\n",
    "    t_max = max(1, t_max)\n",
    "    frac = np.clip(t / t_max, 0.0, 1.0)\n",
    "    return float(a0 + (a1 - a0) * frac)\n",
    "\n",
    "\n",
    "def kl_weight_schedule(t: int, t_max: int, w0: float = 0.1, w1: float = 0.0) -> float:\n",
    "    \"\"\"Linear decay of the KL weight.\"\"\"\n",
    "    t_max = max(1, t_max)\n",
    "    frac = np.clip(t / t_max, 0.0, 1.0)\n",
    "    return float(w0 + (w1 - w0) * frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dfaa83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_seed_length(seed_seq: str, L_local: int, rng: np.random.Generator) -> str:\n",
    "    \"\"\"Trim or pad the seed sequence to length L.\"\"\"\n",
    "    seq = seed_seq.strip().upper()\n",
    "    if len(seq) >= L_local:\n",
    "        return seq[:L_local]\n",
    "    pad = ''.join(rng.choice(list(AA), size=L_local - len(seq)))\n",
    "    return (seq + pad)[:L_local]\n",
    "\n",
    "\n",
    "def init_mixed_logits(seed_seq: str, L_local: int, guided_mask: np.ndarray, seed_ratio: float, rng: Optional[np.random.Generator] = None, rand_scale: float = 1.0) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Initialise logits by blending seed and random draws.\"\"\"\n",
    "    assert len(seed_seq) >= L_local, \"Seed sequence must be at least L residues long.\"\n",
    "    seed_seq_use = seed_seq[:L_local]\n",
    "    seed_oh = seq_to_onehot(seed_seq_use)\n",
    "    seed_logits = onehot_to_logits(seed_oh, sharp=6.0)\n",
    "\n",
    "    rng = np.random.default_rng(None if rng is None else rng.integers(1 << 32))\n",
    "    rand_logits = jnp.array(rng.normal(0.0, rand_scale, size=(L_local, 20)))\n",
    "\n",
    "    mask = jnp.array(guided_mask).reshape(-1, 1).astype(jnp.float32)\n",
    "    pos_seed_ratio = mask * seed_ratio\n",
    "    logits0 = pos_seed_ratio * seed_logits + (1.0 - pos_seed_ratio) * rand_logits\n",
    "    return logits0, seed_logits\n",
    "\n",
    "\n",
    "def blend_logits(current_logits: jnp.ndarray, seed_logits: jnp.ndarray, guided_mask: np.ndarray, seed_ratio: float) -> jnp.ndarray:\n",
    "    \"\"\"Blend current logits with seed logits at guided positions.\"\"\"\n",
    "    mask = jnp.array(guided_mask).reshape(-1, 1).astype(jnp.float32)\n",
    "    blended = mask * (seed_ratio * seed_logits + (1.0 - seed_ratio) * current_logits) + (1.0 - mask) * current_logits\n",
    "    return blended\n",
    "\n",
    "\n",
    "def indices_to_seq(idxs: np.ndarray) -> str:\n",
    "    \"\"\"Convert integer indices to an amino-acid string.\"\"\"\n",
    "    return ''.join(idx_to_aa[int(i)] for i in idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e034e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_binder_softseq(model, softseq: np.ndarray) -> None:\n",
    "    \"\"\"Inject a soft sequence (probabilities) into the binder portion of the model.\"\"\"\n",
    "    binder_len = softseq.shape[0]\n",
    "    total_logits = np.array(model._params[\"seq\"])\n",
    "    binder_start = model._target_len\n",
    "    binder_slice = slice(binder_start, binder_start + binder_len)\n",
    "    logits = np.log(np.clip(softseq, 1e-8, 1.0))\n",
    "    total_logits[:, binder_slice, :] = logits\n",
    "    model._params[\"seq\"] = total_logits\n",
    "    model._tmp = getattr(model, \"_tmp\", {})\n",
    "    model._tmp[\"seq_logits\"] = total_logits\n",
    "\n",
    "\n",
    "def guided_seq_kl(logits: jnp.ndarray, seed_logits: jnp.ndarray, guided_mask: np.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"KL divergence between current and seed distributions on guided sites.\"\"\"\n",
    "    p = jax.nn.softmax(logits, axis=-1)\n",
    "    q = jax.nn.softmax(seed_logits, axis=-1)\n",
    "    mask = jnp.array(guided_mask).reshape(-1, 1).astype(jnp.float32)\n",
    "    p_safe = jnp.clip(p, 1e-8, 1.0)\n",
    "    q_safe = jnp.clip(q, 1e-8, 1.0)\n",
    "    kl = jnp.sum(mask * (p_safe * (jnp.log(p_safe) - jnp.log(q_safe))))\n",
    "    return kl\n",
    "\n",
    "\n",
    "def collect_metrics(model, stage: str, iter_in_stage: int, binder_slice: slice, guided_seed_ratio: Optional[float], kl_weight: float, extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Package per-iteration metrics for downstream logging.\"\"\"\n",
    "    aux = getattr(model, \"aux\", {}) or {}\n",
    "    entry: Dict[str, Any] = {\n",
    "        \"stage\": stage,\n",
    "        \"iter_in_stage\": int(iter_in_stage),\n",
    "        \"global_iter\": int(getattr(model, \"_k\", 0)),\n",
    "        \"guided_seed_ratio\": None if guided_seed_ratio is None else float(guided_seed_ratio),\n",
    "        \"kl_weight\": float(kl_weight),\n",
    "        \"loss_total\": float(aux.get(\"loss\", np.nan)),\n",
    "    }\n",
    "    losses = aux.get(\"losses\", {})\n",
    "    for name, value in losses.items():\n",
    "        entry[f\"loss_{name}\"] = float(value)\n",
    "    if \"plddt\" in aux:\n",
    "        plddt = np.array(aux[\"plddt\"], dtype=float)\n",
    "        binder_plddt = plddt[binder_slice]\n",
    "        entry[\"binder_plddt_mean\"] = float(np.nanmean(binder_plddt))\n",
    "        entry[\"binder_plddt_min\"] = float(np.nanmin(binder_plddt))\n",
    "        entry[\"binder_plddt_max\"] = float(np.nanmax(binder_plddt))\n",
    "        if binder_slice.start > 0:\n",
    "            target_plddt = plddt[:binder_slice.start]\n",
    "            entry[\"target_plddt_mean\"] = float(np.nanmean(target_plddt))\n",
    "    for key in (\"ptm\", \"i_ptm\", \"pae\", \"i_pae\"):\n",
    "        if key in aux:\n",
    "            value = aux[key]\n",
    "            if isinstance(value, (np.ndarray, jnp.ndarray)):\n",
    "                value = np.nanmean(np.array(value, dtype=float))\n",
    "            entry[key] = float(value)\n",
    "    if extra:\n",
    "        entry.update(extra)\n",
    "    return entry\n",
    "\n",
    "\n",
    "def update_best_record(model, best: Dict[str, Any], stage: str, iter_in_stage: int, binder_slice: slice) -> Dict[str, Any]:\n",
    "    \"\"\"Track the best (lowest-loss) design encountered so far.\"\"\"\n",
    "    aux = getattr(model, \"aux\", {}) or {}\n",
    "    current_loss = float(aux.get(\"loss\", np.inf))\n",
    "    if best.get(\"loss\") is None or current_loss < best[\"loss\"]:\n",
    "        if \"seq\" in aux and \"logits\" in aux[\"seq\"]:\n",
    "            seq_logits = np.array(aux[\"seq\"][\"logits\"])\n",
    "        else:\n",
    "            seq_logits = np.array(model._params[\"seq\"])\n",
    "        seq_idx = seq_logits.argmax(-1)\n",
    "        best = {\n",
    "            \"loss\": current_loss,\n",
    "            \"seq\": np.array(seq_idx, dtype=np.int64),\n",
    "            \"aux\": copy.deepcopy(aux),\n",
    "            \"stage\": stage,\n",
    "            \"iter_in_stage\": int(iter_in_stage),\n",
    "            \"global_iter\": int(getattr(model, \"_k\", 0))\n",
    "        }\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4718df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax_np(logits: np.ndarray) -> np.ndarray:\n",
    "    logits = np.asarray(logits, dtype=float)\n",
    "    logits = logits - logits.max(axis=-1, keepdims=True)\n",
    "    exp = np.exp(logits)\n",
    "    return exp / np.clip(exp.sum(axis=-1, keepdims=True), 1e-8, None)\n",
    "def _prepare_seed(seed_input: str, seed_path: Path, template_binder: Optional[Path] = None, binder_chains: Optional[Iterable[str]] = None) -> str:\n",
    "    if seed_input.strip():\n",
    "        return read_fasta_str(seed_input)\n",
    "    if seed_path.exists():\n",
    "        return read_fasta_str(seed_path.read_text())\n",
    "    if template_binder and template_binder.exists():\n",
    "        seq = extract_pdb_sequence(template_binder, binder_chains)\n",
    "        if seq:\n",
    "            return seq\n",
    "    raise ValueError(\"Provide SEED_FASTA, a FASTA file at SEED_FASTA_PATH, or a valid template binder PDB.\")\n",
    "def run_stage4_semigreedy(model, stage_len: int, mut_rate: float, guided_mask: np.ndarray, seed_idxs: np.ndarray, binder_slice: slice, rng: np.random.Generator, bias_stage4: bool, bias_weight: float, metrics: list[Dict[str, Any]], best_record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Semi-greedy refinement with optional seed-aware proposal bias.\"\"\"\n",
    "    binder_len = binder_slice.stop - binder_slice.start\n",
    "    num_mutations = max(1, int(round(mut_rate * binder_len)))\n",
    "    tries_per_iter = max(4, num_mutations * 2)\n",
    "    for local_idx in range(stage_len):\n",
    "        aux = getattr(model, \"aux\", {}) or {}\n",
    "        if \"seq\" in aux and \"logits\" in aux[\"seq\"]:\n",
    "            logits_full = np.array(aux[\"seq\"][\"logits\"], dtype=float)\n",
    "        else:\n",
    "            logits_full = np.array(model._params[\"seq\"], dtype=float)\n",
    "        current_seq = logits_full.argmax(-1)\n",
    "        binder_logits = logits_full[0, binder_slice, :]\n",
    "        binder_seq = current_seq[:, binder_slice].copy()\n",
    "        current_loss = float(aux.get(\"loss\", np.inf))\n",
    "        if \"plddt\" in aux:\n",
    "            binder_plddt = np.array(aux[\"plddt\"], dtype=float)[binder_slice]\n",
    "            pos_weights = 1.0 - np.clip(binder_plddt, 0.0, 1.0)\n",
    "        else:\n",
    "            pos_weights = np.ones(binder_len, dtype=float)\n",
    "        if not np.isfinite(pos_weights).any():\n",
    "            pos_weights = np.ones(binder_len, dtype=float)\n",
    "        pos_weights = np.clip(pos_weights, 1e-3, None)\n",
    "        pos_weights /= pos_weights.sum()\n",
    "        proposals = []\n",
    "        for _ in range(tries_per_iter):\n",
    "            mutate_positions = rng.choice(binder_len, size=num_mutations, replace=False, p=pos_weights)\n",
    "            mutant = binder_seq.copy()\n",
    "            for pos in mutate_positions:\n",
    "                probs = _softmax_np(binder_logits[pos])\n",
    "                probs[int(mutant[0, pos])] = 0.0\n",
    "                if bias_stage4 and guided_mask[pos]:\n",
    "                    probs = (1.0 - bias_weight) * probs\n",
    "                    probs[int(seed_idxs[pos])] += bias_weight\n",
    "                probs = probs / np.clip(probs.sum(), 1e-8, None)\n",
    "                mutant[0, pos] = rng.choice(20, p=probs)\n",
    "            candidate = current_seq.copy()\n",
    "            candidate[:, binder_slice] = mutant\n",
    "            aux_candidate = model.predict(seq=candidate, return_aux=True, verbose=False, dropout=False, hard=True, soft=False, temp=1e-2, models=[0], num_models=1, sample_models=False)\n",
    "            proposals.append((float(aux_candidate[\"loss\"]), candidate.copy(), copy.deepcopy(model.aux)))\n",
    "        best_loss, best_seq_full, best_aux = min(proposals, key=lambda x: x[0])\n",
    "        if best_loss <= current_loss or not np.isfinite(current_loss):\n",
    "            model.set_seq(seq=best_seq_full, bias=model._inputs.get(\"bias\"), set_state=False)\n",
    "            model.aux = best_aux\n",
    "        metrics.append(collect_metrics(model, stage=\"stage4\", iter_in_stage=local_idx, binder_slice=binder_slice, guided_seed_ratio=None, kl_weight=0.0, extra={\"temp\": 1e-2, \"accepted_loss\": float(model.aux.get(\"loss\", np.nan))}))\n",
    "        best_record = update_best_record(model, best_record, stage=\"stage4\", iter_in_stage=local_idx, binder_slice=binder_slice)\n",
    "        model._k = int(getattr(model, \"_k\", 0)) + 1\n",
    "    return best_record\n",
    "def run_hybrid_design(\n",
    "    target_pdb_path: Path,\n",
    "    binder_len: int,\n",
    "    seed_fasta: str,\n",
    "    template_binder_path: Optional[Path] = None,\n",
    "    stage_params: Optional[Dict[str, int]] = None,\n",
    "    init_params: Optional[Dict[str, float]] = None,\n",
    "    kl_params: Optional[Dict[str, float]] = None,\n",
    "    guided_fraction: float = 0.5,\n",
    "    mut_rate: float = 0.05,\n",
    "    use_kl_prior: bool = True,\n",
    "    bias_stage4: bool = True,\n",
    "    rng_seed: int = 0,\n",
    "    run_tag: str = \"hybrid\",\n",
    "    output_root: Path = OUTPUT_BASE,\n",
    "    baseline_mode: str = \"hybrid\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Execute the hybrid AF2 binder hallucination workflow.\"\"\"\n",
    "    target_pdb_path = Path(target_pdb_path)\n",
    "    if not target_pdb_path.exists():\n",
    "        raise FileNotFoundError(f\"Target PDB not found: {target_pdb_path}\")\n",
    "    template_binder_path = Path(template_binder_path) if template_binder_path else None\n",
    "    stage_defaults = {\n",
    "        \"STAGE1_ITERS\": 50,\n",
    "        \"STAGE1_EXTRA\": 25,\n",
    "        \"STAGE2_ITERS\": 45,\n",
    "        \"STAGE3_ITERS\": 5,\n",
    "        \"STAGE4_ITERS\": 15,\n",
    "    }\n",
    "    if stage_params:\n",
    "        stage_defaults.update(stage_params)\n",
    "    stage_params = stage_defaults\n",
    "    init_defaults = {\"seed_ratio_start\": 0.9, \"seed_ratio_end\": 0.2}\n",
    "    if init_params:\n",
    "        init_defaults.update({\"seed_ratio_start\": init_params.get(\"seed_ratio_start\", 0.9), \"seed_ratio_end\": init_params.get(\"seed_ratio_end\", 0.2)})\n",
    "    init_params = init_defaults\n",
    "    kl_defaults = {\"w_start\": 0.1, \"w_end\": 0.0}\n",
    "    if kl_params:\n",
    "        kl_defaults.update({\"w_start\": kl_params.get(\"w_start\", 0.1), \"w_end\": kl_params.get(\"w_end\", 0.0)})\n",
    "    kl_params = kl_defaults\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    seed_seq_raw = read_fasta_str(seed_fasta)\n",
    "    if not seed_seq_raw:\n",
    "        raise ValueError(\"Seed FASTA is empty after parsing.\")\n",
    "    if baseline_mode == \"random\":\n",
    "        guided_fraction = 0.0\n",
    "        init_params[\"seed_ratio_start\"] = 0.0\n",
    "        init_params[\"seed_ratio_end\"] = 0.0\n",
    "    elif baseline_mode == \"seed\":\n",
    "        guided_fraction = 1.0\n",
    "        init_params[\"seed_ratio_start\"] = 1.0\n",
    "        init_params[\"seed_ratio_end\"] = 1.0\n",
    "    clear_mem()\n",
    "    model = mk_afdesign_model(protocol=\"binder\", data_dir=str(AF_PARAMS_DIR))\n",
    "    model._model_names = model._model_names[:1] or [\"model_1_multimer_v3\"]\n",
    "    model.set_opt(num_models=1, sample_models=False)\n",
    "    prep_kwargs = {\"pdb_filename\": str(target_pdb_path), \"binder_len\": binder_len, \"target_chain\": TARGET_CHAIN}\n",
    "    if template_binder_path and template_binder_path.exists():\n",
    "        prep_kwargs[\"binder_pdb\"] = str(template_binder_path)\n",
    "    model.prep_inputs(**prep_kwargs)\n",
    "    binder_start = model._target_len\n",
    "    total_seq_len = int(model._params[\"seq\"].shape[1])\n",
    "    binder_len_model = total_seq_len - binder_start\n",
    "    print(f\"target_len={binder_start}, total_seq_len={total_seq_len}, binder_slots={binder_len_model}\")\n",
    "    total_seq_len = model._params[\"seq\"].shape[1]\n",
    "    if binder_len_model != binder_len:\n",
    "        print(f\"Adjusted binder length from {binder_len} to {binder_len_model} to match AF2 model inputs.\")\n",
    "    binder_len = binder_len_model\n",
    "    seed_seq = ensure_seed_length(seed_seq_raw, binder_len, rng)\n",
    "    guided_mask = make_guided_mask(binder_len, fraction=guided_fraction, rng=rng)\n",
    "    logits0, seed_logits = init_mixed_logits(seed_seq, binder_len, guided_mask, seed_ratio=init_params[\"seed_ratio_start\"], rng=rng)\n",
    "    logits0_np = np.array(np.asarray(logits0), dtype=np.float32)\n",
    "    print(f\"logits0_np shape: {logits0_np.shape}, seed length: {len(seed_seq)}\")\n",
    "    seed_logits = jnp.array(seed_logits)\n",
    "    seed_idxs = np.array([aa_to_idx[a] for a in seed_seq], dtype=np.int64)\n",
    "    binder_slice = slice(binder_start, binder_start + binder_len)\n",
    "    seq_logits_full = np.array(model._params[\"seq\"], dtype=float)\n",
    "    seq_logits_full[:, binder_slice, :] = logits0_np\n",
    "    model._params[\"seq\"] = seq_logits_full\n",
    "    soft_init = np.array(jax.nn.softmax(logits0, axis=-1))\n",
    "    set_binder_softseq(model, soft_init)\n",
    "    model.set_opt(soft=1.0, hard=0.0, temp=1.0, dropout=True, num_models=1, sample_models=False)\n",
    "    model._callbacks[\"design\"][\"pre\"] = []\n",
    "    metrics: list[Dict[str, Any]] = []\n",
    "    best_record: Dict[str, Any] = {\"loss\": None}\n",
    "    if use_kl_prior:\n",
    "        kl_loss_fn = jax.jit(lambda x: guided_seq_kl(x, seed_logits, guided_mask))\n",
    "        kl_grad_fn = jax.jit(jax.grad(lambda x: guided_seq_kl(x, seed_logits, guided_mask)))\n",
    "    else:\n",
    "        kl_loss_fn = None\n",
    "        kl_grad_fn = None\n",
    "    stage1_total = stage_params[\"STAGE1_ITERS\"] + stage_params[\"STAGE1_EXTRA\"]\n",
    "    print(f\"Stage 1/4 (logits): {stage1_total} iterations\")\n",
    "    kl_total = stage1_total + max(0, stage_params[\"STAGE2_ITERS\"] // 2)\n",
    "    kl_counter = 0\n",
    "    for local_idx in range(stage1_total):\n",
    "        print(f\"Stage 1 progress {local_idx+1}/{stage1_total}\", end=\"\\r\", flush=True)\n",
    "        seed_ratio = guided_seed_ratio_schedule(local_idx, max(1, stage1_total - 1), init_params[\"seed_ratio_start\"], init_params[\"seed_ratio_end\"])\n",
    "        kl_weight = 0.0\n",
    "        if use_kl_prior and kl_loss_fn is not None:\n",
    "            kl_weight = kl_weight_schedule(kl_counter, max(1, kl_total - 1), kl_params[\"w_start\"], kl_params[\"w_end\"])\n",
    "            kl_counter += 1\n",
    "        def pre_cb(mod, seed_ratio_val=seed_ratio):\n",
    "            logits_now = jnp.array(mod._params[\"seq\"][0, binder_slice, :])\n",
    "            blended = blend_logits(logits_now, seed_logits, guided_mask, seed_ratio_val)\n",
    "            seq_all = np.array(mod._params[\"seq\"], dtype=float)\n",
    "            seq_all[0, binder_slice, :] = np.array(blended)\n",
    "            mod._params[\"seq\"] = seq_all\n",
    "        def post_cb(mod, seed_ratio_val=seed_ratio, kl_weight_val=kl_weight):\n",
    "            if use_kl_prior and kl_loss_fn is not None and kl_weight_val > 0:\n",
    "                binder_logits = jnp.array(mod._params[\"seq\"][0, binder_slice, :])\n",
    "                kl_val = kl_loss_fn(binder_logits)\n",
    "                grad = kl_grad_fn(binder_logits)\n",
    "                mod.aux[\"grad\"][\"seq\"][0, binder_slice, :] += np.array(kl_weight_val * grad)\n",
    "                mod.aux.setdefault(\"log\", {})[\"guided_kl\"] = float(kl_val)\n",
    "                mod.aux[\"log\"][\"kl_weight\"] = float(kl_weight_val)\n",
    "            mod.aux.setdefault(\"log\", {})[\"guided_seed_ratio\"] = float(seed_ratio_val)\n",
    "        model._callbacks[\"design\"][\"pre\"] = [pre_cb]\n",
    "        model.step(callback=post_cb, save_best=True, models=[0], num_models=1, sample_models=False)\n",
    "        model._callbacks[\"design\"][\"pre\"] = []\n",
    "        metrics.append(collect_metrics(model, stage=\"stage1\", iter_in_stage=local_idx, binder_slice=binder_slice, guided_seed_ratio=seed_ratio, kl_weight=kl_weight))\n",
    "        best_record = update_best_record(model, best_record, stage=\"stage1\", iter_in_stage=local_idx, binder_slice=binder_slice)\n",
    "    print(f\"Stage 2/4 (softmax anneal): {stage_params['STAGE2_ITERS']} iterations\")\n",
    "    for local_idx in range(stage_params[\"STAGE2_ITERS\"]):\n",
    "        print(f\"Stage 2 progress {local_idx+1}/{stage_params['STAGE2_ITERS']}\", end=\"\\r\", flush=True)\n",
    "        frac = np.clip(local_idx / max(1, stage_params[\"STAGE2_ITERS\"] - 1), 0.0, 1.0)\n",
    "        temp = 1.0 + (0.05 - 1.0) * frac\n",
    "        kl_weight = 0.0\n",
    "        apply_kl = use_kl_prior and kl_loss_fn is not None and local_idx < (stage_params[\"STAGE2_ITERS\"] // 2)\n",
    "        if apply_kl:\n",
    "            kl_weight = kl_weight_schedule(kl_counter, max(1, kl_total - 1), kl_params[\"w_start\"], kl_params[\"w_end\"])\n",
    "            kl_counter += 1\n",
    "        def post_cb(mod, temp_val=temp, kl_weight_val=kl_weight):\n",
    "            mod.aux.setdefault(\"log\", {})[\"temp\"] = float(temp_val)\n",
    "            if apply_kl and kl_weight_val > 0:\n",
    "                binder_logits = jnp.array(mod._params[\"seq\"][0, binder_slice, :])\n",
    "                kl_val = kl_loss_fn(binder_logits)\n",
    "                grad = kl_grad_fn(binder_logits)\n",
    "                mod.aux[\"grad\"][\"seq\"][0, binder_slice, :] += np.array(kl_weight_val * grad)\n",
    "                mod.aux[\"log\"][\"guided_kl\"] = float(kl_val)\n",
    "                mod.aux[\"log\"][\"kl_weight\"] = float(kl_weight_val)\n",
    "        model.set_opt(temp=float(temp), num_models=1, sample_models=False)\n",
    "        model.step(callback=post_cb, save_best=True, models=[0], num_models=1, sample_models=False)\n",
    "        metrics.append(collect_metrics(model, stage=\"stage2\", iter_in_stage=local_idx, binder_slice=binder_slice, guided_seed_ratio=None, kl_weight=kl_weight, extra={\"temp\": float(temp)}))\n",
    "        best_record = update_best_record(model, best_record, stage=\"stage2\", iter_in_stage=local_idx, binder_slice=binder_slice)\n",
    "    model.set_opt(soft=0.0, hard=1.0, temp=1e-2, dropout=False, num_models=1, sample_models=False)\n",
    "    print(f\"Stage 3/4 (hard/STE): {stage_params['STAGE3_ITERS']} iterations\")\n",
    "    for local_idx in range(stage_params[\"STAGE3_ITERS\"]):\n",
    "        print(f\"Stage 3 progress {local_idx+1}/{stage_params['STAGE3_ITERS']}\", end=\"\\r\", flush=True)\n",
    "        def post_cb(mod):\n",
    "            mod.aux.setdefault(\"log\", {})[\"temp\"] = 1e-2\n",
    "        model.step(callback=post_cb, save_best=True, models=[0], num_models=1, sample_models=False)\n",
    "        metrics.append(collect_metrics(model, stage=\"stage3\", iter_in_stage=local_idx, binder_slice=binder_slice, guided_seed_ratio=None, kl_weight=0.0, extra={\"temp\": 1e-2}))\n",
    "        best_record = update_best_record(model, best_record, stage=\"stage3\", iter_in_stage=local_idx, binder_slice=binder_slice)\n",
    "    if not getattr(model, \"aux\", None):\n",
    "        model.predict(return_aux=True, verbose=False, dropout=False, hard=True, soft=False)\n",
    "    print(f\"Stage 4/4 (semi-greedy): {stage_params['STAGE4_ITERS']} iterations\")\n",
    "    best_record = run_stage4_semigreedy(\n",
    "        model=model,\n",
    "        stage_len=stage_params[\"STAGE4_ITERS\"],\n",
    "        mut_rate=mut_rate,\n",
    "        guided_mask=np.array(guided_mask, dtype=bool),\n",
    "        seed_idxs=seed_idxs,\n",
    "        binder_slice=binder_slice,\n",
    "        rng=rng,\n",
    "        bias_stage4=bias_stage4,\n",
    "        bias_weight=float(STAGE4_SEED_BIAS),\n",
    "        metrics=metrics,\n",
    "        best_record=best_record,\n",
    "    )\n",
    "    run_dir = output_root / run_tag\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_path = run_dir / \"metrics.csv\"\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    best_seq_full = np.array(best_record[\"seq\"], dtype=np.int64)\n",
    "    binder_idx = best_seq_full[:, binder_slice]\n",
    "    binder_seq = indices_to_seq(binder_idx[0])\n",
    "    fasta_path = run_dir / \"best_binder.fasta\"\n",
    "    with open(fasta_path, \"w\") as handle:\n",
    "        handle.write(\">designed_binder\\n\")\n",
    "        handle.write(binder_seq + \"\\n\")\n",
    "    pdb_path = run_dir / \"best_complex.pdb\"\n",
    "    model.set_seq(seq=best_seq_full, bias=model._inputs.get(\"bias\"), set_state=False)\n",
    "    model.save_pdb(filename=str(pdb_path), get_best=False, aux=best_record[\"aux\"])\n",
    "    binder_plddt = np.nan\n",
    "    if \"plddt\" in best_record[\"aux\"]:\n",
    "        binder_plddt = float(np.nanmean(np.array(best_record[\"aux\"][\"plddt\"], dtype=float)[binder_slice]))\n",
    "    result = {\n",
    "        \"binder_len\": binder_len,\n",
    "        \"model\": model,\n",
    "        \"metrics_df\": metrics_df,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"fasta_path\": fasta_path,\n",
    "        \"best_pdb_path\": pdb_path,\n",
    "        \"best_binder_seq\": binder_seq,\n",
    "        \"best_loss\": float(best_record[\"loss\"]),\n",
    "        \"best_plddt\": binder_plddt,\n",
    "        \"run_tag\": run_tag,\n",
    "        \"guided_mask\": np.array(guided_mask, dtype=bool),\n",
    "        \"seed_seq\": seed_seq,\n",
    "        \"binder_slice\": binder_slice,\n",
    "        \"best_record\": best_record,\n",
    "        \"target_chain\": prep_kwargs.get(\"target_chain\", \"A\"),\n",
    "        \"binder_chain\": prep_kwargs.get(\"binder_chain\", \"B\"),\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_len=130, total_seq_len=229, binder_slots=99\n",
      "Adjusted binder length from 229 to 99 to match AF2 model inputs.\n",
      "logits0_np shape: (99, 20), seed length: 99\n",
      "Stage 1/4 (logits): 75 iterations\n",
      "Stage 1 progress 1/75\r"
     ]
    }
   ],
   "source": [
    "seed_source = _prepare_seed(SEED_FASTA, SEED_FASTA_PATH, template_binder=TEMPLATE_BINDER_PDB_PATH, binder_chains=TEMPLATE_BINDER_CHAINS)\n",
    "template_path = Path(TEMPLATE_BINDER_PDB_PATH)\n",
    "if not str(template_path).strip() or not template_path.exists():\n",
    "    template_path = None\n",
    "binder_len = L if L is not None else len(seed_source)\n",
    "stage_cfg = {\n",
    "    \"STAGE1_ITERS\": STAGE1_ITERS,\n",
    "    \"STAGE1_EXTRA\": STAGE1_EXTRA,\n",
    "    \"STAGE2_ITERS\": STAGE2_ITERS,\n",
    "    \"STAGE3_ITERS\": STAGE3_ITERS,\n",
    "    \"STAGE4_ITERS\": STAGE4_ITERS,\n",
    "}\n",
    "init_cfg = {\"seed_ratio_start\": GUIDED_SEED_RATIO_START, \"seed_ratio_end\": GUIDED_SEED_RATIO_END}\n",
    "kl_cfg = {\"w_start\": KL_W_START, \"w_end\": KL_W_END}\n",
    "run_root = OUTPUT_BASE / RUN_NAME\n",
    "run_root.mkdir(parents=True, exist_ok=True)\n",
    "HYBRID_RESULT = run_hybrid_design(\n",
    "    target_pdb_path=TARGET_PDB_PATH,\n",
    "    binder_len=binder_len,\n",
    "    seed_fasta=seed_source,\n",
    "    template_binder_path=template_path,\n",
    "    stage_params=stage_cfg,\n",
    "    init_params=init_cfg,\n",
    "    kl_params=kl_cfg,\n",
    "    guided_fraction=GUIDED_FRACTION,\n",
    "    mut_rate=MUT_RATE,\n",
    "    use_kl_prior=USE_KL_PRIOR,\n",
    "    bias_stage4=BIAS_STAGE4_PROPOSALS,\n",
    "    rng_seed=GLOBAL_RNG_SEED,\n",
    "    run_tag=\"hybrid\",\n",
    "    output_root=run_root,\n",
    "    baseline_mode=\"hybrid\",\n",
    ")\n",
    "binder_len = HYBRID_RESULT[\"binder_len\"]\n",
    "print(\"Hybrid design complete.\")\n",
    "print(f\"  Best loss: {HYBRID_RESULT['best_loss']:.4f}\")\n",
    "print(f\"  Binder pLDDT (mean): {HYBRID_RESULT['best_plddt']:.3f}\")\n",
    "print(f\"  Sequence saved to: {HYBRID_RESULT['fasta_path']}\")\n",
    "print(f\"  Complex PDB saved to: {HYBRID_RESULT['best_pdb_path']}\")\n",
    "BASELINE_RESULTS = []\n",
    "if RUN_BASELINES:\n",
    "    modes = []\n",
    "    if RUN_RANDOM_BASELINE:\n",
    "        modes.append(\"random\")\n",
    "    if RUN_SEED_BASELINE:\n",
    "        modes.append(\"seed\")\n",
    "    for offset, mode in enumerate(modes, start=1):\n",
    "        result = run_hybrid_design(\n",
    "            target_pdb_path=TARGET_PDB_PATH,\n",
    "            binder_len=binder_len,\n",
    "            seed_fasta=seed_source,\n",
    "            template_binder_path=template_path,\n",
    "            stage_params=stage_cfg,\n",
    "            init_params=init_cfg,\n",
    "            kl_params=kl_cfg,\n",
    "            guided_fraction=GUIDED_FRACTION,\n",
    "            mut_rate=MUT_RATE,\n",
    "            use_kl_prior=USE_KL_PRIOR,\n",
    "            bias_stage4=BIAS_STAGE4_PROPOSALS,\n",
    "            rng_seed=GLOBAL_RNG_SEED + offset,\n",
    "            run_tag=mode,\n",
    "            output_root=run_root,\n",
    "            baseline_mode=mode,\n",
    "        )\n",
    "        BASELINE_RESULTS.append(result)\n",
    "        print(f\"Baseline '{mode}' complete -> loss {result['best_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "pdb_view = py3Dmol.view(width=640, height=480)\n",
    "with open(str(HYBRID_RESULT['best_pdb_path'])) as handle:\n",
    "    pdb_view.addModel(handle.read(), 'pdb')\n",
    "\n",
    "binder_chain = HYBRID_RESULT.get('binder_chain', 'B')\n",
    "target_chain = HYBRID_RESULT.get('target_chain', 'A')\n",
    "\n",
    "pdb_view.setStyle({'chain': target_chain}, {'cartoon': {'color': 'white'}})\n",
    "pdb_view.setStyle({'chain': binder_chain}, {'cartoon': {'color': 'rainbow'}})\n",
    "pdb_view.zoomTo()\n",
    "pdb_view.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = [{\n",
    "    'run': 'hybrid',\n",
    "    'loss': HYBRID_RESULT['best_loss'],\n",
    "    'binder_plddt': HYBRID_RESULT['best_plddt'],\n",
    "    'fasta_path': str(HYBRID_RESULT['fasta_path']),\n",
    "    'pdb_path': str(HYBRID_RESULT['best_pdb_path']),\n",
    "}]\n",
    "for res in BASELINE_RESULTS:\n",
    "    summary_rows.append({\n",
    "        'run': res['run_tag'],\n",
    "        'loss': res['best_loss'],\n",
    "        'binder_plddt': res['best_plddt'],\n",
    "        'fasta_path': str(res['fasta_path']),\n",
    "        'pdb_path': str(res['best_pdb_path']),\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "display(summary_df)\n",
    "summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BindCraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}